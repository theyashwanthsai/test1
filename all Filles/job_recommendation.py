# -*- coding: utf-8 -*-
"""job-recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Pd6ArzKkreVGHNFbdpfLEgdqnUcupss

#### This time, I'm also working on the Google Job dataset. While this time, I would like to build a simple recommendation system based on the scenario of looking for a position and finding similar openings for users this time

![google](http://img.technews.tw/wp-content/uploads/2015/09/Google-logo_1.jpg)

# Outline

## Recommendation System

- [EDA](#0)
    * I'll do simple exploratory on the data structure and values
- [Modeling](#1)
    * I'll start to test out vectorize text and find similar positions based on job description
- [Finalizing](#2)
    * Will also consider requirements in this part

## [Text Clustering](#Cluster)
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
plt.style.use('ggplot')

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('job_skills.csv')

"""## [EDA](#0)

**First, I would like to know more about the data**

- Starting from the columns
- Then, the text pattern in columns
- Finally, the correlation between different positions
"""

df.head()

# I modify the column name so that I can use df dot column name more easily
df = df.rename(columns={'Minimum Qualifications': 'Minimum_Qualifications', 'Preferred Qualifications': 'Preferred_Qualifications'})

df.Company.value_counts()

df.Category.value_counts()

df.Location.value_counts()[:10]

df['Country'] = df['Location'].apply(lambda x : x.split(',')[-1])

df.Country.value_counts()[:10]

pd.isnull(df).sum()

df = df.dropna(how='any',axis='rows')

"""## [Modeling](#1)"""

# Perform the necessary imports for similarity
from sklearn.decomposition import NMF
from sklearn.preprocessing import Normalizer, MaxAbsScaler
from sklearn.pipeline import make_pipeline


scaler = MaxAbsScaler()

model = NMF(n_components=100)

normalizer = Normalizer()

# Create a pipeline: pipeline
pipeline = make_pipeline(scaler,model,normalizer)

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
vectors_Responsibilities = vectorizer.fit_transform(df['Responsibilities'])

Responsibilities = pipeline.fit_transform(vectors_Responsibilities)

df_Responsibilities = pd.DataFrame(Responsibilities,index=df['Title'])

df_Responsibilities.head(2)

pd.set_option('display.max_colwidth', -1)
print(df[df.Title.str.contains('Data Scientist')]['Title'])

Position = df_Responsibilities.loc['Customer Experience Data Scientist, Google Cloud Support']

similarities_1 = df_Responsibilities.dot(Position)

similarities_1[:3]

print(similarities_1.nlargest())

"""### Let's see if the role is similar and ideal as an alternative."""

df[np.isin(df['Title'],similarities_1.nlargest().index.tolist())].head()

type(similarities_1)

"""### In my opinion, the role is a good alternative choice while the requirement could be a blocker. So let's also consider the part of requirements."""

vectorizer_Requirements = TfidfVectorizer()
vectors_Requirements = vectorizer_Requirements.fit_transform(df['Minimum_Qualifications'])

Requirements = pipeline.fit_transform(vectors_Requirements)

df_Requirementss = pd.DataFrame(Requirements,index=df['Title'])

Position = df_Requirementss.loc['Customer Experience Data Scientist, Google Cloud Support']

similarities_2 = df_Responsibilities.dot(Position)

print(similarities_2.nlargest())

"""Though this looks a bit weird, let's see how we put responsibilities and requirements together first."""

similarities_1

similarities_1.rename("similarity")
similarities_2.rename("similarity")

similarities_1.to_frame().join(similarities_2.to_frame(),lsuffix='1')

similarities_overall = (2 * similarities_1) + similarities_2

print(similarities_overall.nlargest())

df[np.isin(df['Title'],similarities_overall.nlargest(3).index.tolist())].head()

"""### The result is not bad! Though one of the alternative position looks more emphasize soft skills part while another is similar in terms of the hard skills part, I think they both look like a good choice as well.

## [Text Clustering](#Clustering)

#### The purpose of this part is aiming at finding the relevant words, skills, requirements across different roles using Cluster Analysis instead of Word Cloud in my previous project.
"""

from scipy.cluster.vq import kmeans, vq
from numpy import random

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import string
#from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS



string.punctuation

stop_words_0 = set(stopwords.words('english'))
stop_words = ['and', 'in', 'of', 'or', 'with','to','on','a']

def remove_noise(text):
    tokens = word_tokenize(text)
    clean_tokens = []
    lemmatizer=WordNetLemmatizer()
    for token in tokens:
        token = re.sub('[!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~]+', '', token)
        token = lemmatizer.lemmatize(token.lower())
        if len(token) > 1 and token not in stop_words_0 and token not in stop_words:
            clean_tokens.append(token)

    return clean_tokens

# Initialize TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=100,tokenizer=remove_noise)

# Use the .fit_transform() method on the list plots
tfidf_matrix = tfidf_vectorizer.fit_transform(df['Minimum_Qualifications'])

random.seed = 123

distortions = []
num_clusters = range(2, 25)

# Create a list of distortions from the kmeans function
for i in num_clusters:
    cluster_centers, distortion = kmeans(tfidf_matrix.todense(),i)
    distortions.append(distortion)

# Create a data frame with two lists - num_clusters, distortions
elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})

# Creat a line plot of num_clusters and distortions
sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)
plt.xticks(num_clusters)
plt.title('Clusters and Distortions')
plt.show()

cluster_centers, distortion = kmeans(tfidf_matrix.todense(),13)

# Generate terms from the tfidf_vectorizer object
terms = tfidf_vectorizer.get_feature_names()

for i in range(13):
    # Sort the terms and print top 10 terms
    center_terms = dict(zip(terms, list(cluster_centers[i])))
    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)
    print(sorted_terms[:5])

"""With the groups of words, I can tell different groups are from different fields of the positions."""

import pickle
pickle.dump(tfidf_vectorizer, open('tfidf_vectorizer.pkl', 'wb'))
pickle.dump(cluster_centers, open('cluster_centers.pkl', 'wb'))

